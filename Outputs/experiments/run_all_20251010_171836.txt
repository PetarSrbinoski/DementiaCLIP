Run timestamp: 20251010_171836
Base dir: C:\Users\petar\PycharmProjects\DementiaCLIP
Python: C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Scripts\python.exe


===== MODE: vision_only =====
============================================================
[Hardware Environment Check]
============================================================
CUDA device count: 1
  - GPU 0: NVIDIA GeForce RTX 4050 Laptop GPU
Using device: NVIDIA GeForce RTX 4050 Laptop GPU
============================================================

Starting training: vision_only on cuda


--- Fold 1/5 ---
Initializing model and data...
Fine-tuning CLIP: No (frozen for 3 epochs)
Batch size: 16
Trainable params: 132,866
Epoch 1/16 - Train Loss: 0.6235, Val Loss: 0.9202 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.3192
Epoch 2/16 - Train Loss: 0.6199, Val Loss: 0.9231 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.3288
Epoch 3/16 - Train Loss: 0.6743, Val Loss: 0.8063 | Acc: 0.3559, F1: 0.2324, ROC-AUC: 0.4032
Epoch 4: Partially unfroze CLIP (last block + LayerNorms).
Epoch 4/16 - Train Loss: 0.6438, Val Loss: 0.8446 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5013
Epoch 5/16 - Train Loss: 0.6502, Val Loss: 0.8419 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5224
Epoch 6/16 - Train Loss: 0.6067, Val Loss: 0.8160 | Acc: 0.3390, F1: 0.1995, ROC-AUC: 0.4910
Epoch 7/16 - Train Loss: 0.5664, Val Loss: 0.9468 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5058
Epoch 8/16 - Train Loss: 0.6395, Val Loss: 0.8363 | Acc: 0.3559, F1: 0.2324, ROC-AUC: 0.4545
Epoch 9/16 - Train Loss: 0.6116, Val Loss: 0.8093 | Acc: 0.3898, F1: 0.2943, ROC-AUC: 0.4564
Epoch 10/16 - Train Loss: 0.6031, Val Loss: 0.8086 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4615
Epoch 11/16 - Train Loss: 0.6191, Val Loss: 0.7912 | Acc: 0.4407, F1: 0.3927, ROC-AUC: 0.4609
Epoch 12/16 - Train Loss: 0.5743, Val Loss: 0.8270 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4596
Epoch 13/16 - Train Loss: 0.5950, Val Loss: 0.8482 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4615
Epoch 14/16 - Train Loss: 0.6312, Val Loss: 0.8348 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4603
Epoch 15/16 - Train Loss: 0.5425, Val Loss: 0.8366 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4583
Epoch 16/16 - Train Loss: 0.5759, Val Loss: 0.8394 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4596
Fold 1 - Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.4596

--- Fold 2/5 ---
Initializing model and data...
Fine-tuning CLIP: No (frozen for 3 epochs)
Batch size: 16
Trainable params: 132,866
Epoch 1/16 - Train Loss: 0.6623, Val Loss: 0.8284 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5840
Epoch 2/16 - Train Loss: 0.7021, Val Loss: 0.8177 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.6071
Epoch 3/16 - Train Loss: 0.6701, Val Loss: 0.7375 | Acc: 0.4407, F1: 0.3927, ROC-AUC: 0.5923
Epoch 4: Partially unfroze CLIP (last block + LayerNorms).
Epoch 4/16 - Train Loss: 0.6477, Val Loss: 0.8836 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.6077
Epoch 5/16 - Train Loss: 0.6541, Val Loss: 0.7334 | Acc: 0.4576, F1: 0.4174, ROC-AUC: 0.5865
Epoch 6/16 - Train Loss: 0.6233, Val Loss: 0.8480 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5987
Epoch 7/16 - Train Loss: 0.6491, Val Loss: 0.8478 | Acc: 0.3559, F1: 0.2069, ROC-AUC: 0.5865
Epoch 8/16 - Train Loss: 0.6373, Val Loss: 0.7573 | Acc: 0.4576, F1: 0.4044, ROC-AUC: 0.5840
Epoch 9/16 - Train Loss: 0.6022, Val Loss: 0.8643 | Acc: 0.3898, F1: 0.2728, ROC-AUC: 0.5872
Epoch 10/16 - Train Loss: 0.6179, Val Loss: 0.8550 | Acc: 0.3898, F1: 0.2728, ROC-AUC: 0.5776
Epoch 11/16 - Train Loss: 0.6460, Val Loss: 0.7757 | Acc: 0.4237, F1: 0.3335, ROC-AUC: 0.5872
Epoch 12/16 - Train Loss: 0.6222, Val Loss: 0.7656 | Acc: 0.4407, F1: 0.3783, ROC-AUC: 0.5827
Epoch 13/16 - Train Loss: 0.5843, Val Loss: 0.8040 | Acc: 0.4237, F1: 0.3335, ROC-AUC: 0.5833
Early stopping at epoch 13
Fold 2 - Acc: 0.4237, F1: 0.3335, ROC-AUC: 0.5833

--- Fold 3/5 ---
Initializing model and data...
Fine-tuning CLIP: No (frozen for 3 epochs)
Batch size: 16
Trainable params: 132,866
Epoch 1/16 - Train Loss: 0.6403, Val Loss: 0.9161 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.4692
Epoch 2/16 - Train Loss: 0.6664, Val Loss: 0.7967 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.4904
Epoch 3/16 - Train Loss: 0.6321, Val Loss: 0.8412 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5269
Epoch 4: Partially unfroze CLIP (last block + LayerNorms).
Epoch 4/16 - Train Loss: 0.6519, Val Loss: 0.7742 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5391
Epoch 5/16 - Train Loss: 0.5858, Val Loss: 0.9083 | Acc: 0.3390, F1: 0.1716, ROC-AUC: 0.5936
Epoch 6/16 - Train Loss: 0.6450, Val Loss: 0.7836 | Acc: 0.3729, F1: 0.2406, ROC-AUC: 0.5942
Epoch 7/16 - Train Loss: 0.6117, Val Loss: 0.7889 | Acc: 0.3559, F1: 0.2324, ROC-AUC: 0.5853
Epoch 8/16 - Train Loss: 0.6215, Val Loss: 0.7893 | Acc: 0.3729, F1: 0.2640, ROC-AUC: 0.5718
Epoch 9/16 - Train Loss: 0.5820, Val Loss: 0.8372 | Acc: 0.3729, F1: 0.2406, ROC-AUC: 0.5692
Epoch 10/16 - Train Loss: 0.6276, Val Loss: 0.7950 | Acc: 0.3898, F1: 0.2943, ROC-AUC: 0.5679
Epoch 11/16 - Train Loss: 0.6100, Val Loss: 0.7620 | Acc: 0.4407, F1: 0.3927, ROC-AUC: 0.5641
Epoch 12/16 - Train Loss: 0.5762, Val Loss: 0.7859 | Acc: 0.4407, F1: 0.3783, ROC-AUC: 0.5679
Epoch 13/16 - Train Loss: 0.5881, Val Loss: 0.8073 | Acc: 0.4068, F1: 0.3233, ROC-AUC: 0.5654
Epoch 14/16 - Train Loss: 0.5638, Val Loss: 0.8042 | Acc: 0.4237, F1: 0.3513, ROC-AUC: 0.5647
Epoch 15/16 - Train Loss: 0.6355, Val Loss: 0.7989 | Acc: 0.4237, F1: 0.3513, ROC-AUC: 0.5641
Epoch 16/16 - Train Loss: 0.6047, Val Loss: 0.7978 | Acc: 0.4407, F1: 0.3783, ROC-AUC: 0.5660
Fold 3 - Acc: 0.4407, F1: 0.3783, ROC-AUC: 0.5660

--- Fold 4/5 ---
Initializing model and data...
Fine-tuning CLIP: No (frozen for 3 epochs)
Batch size: 16
Trainable params: 132,866
Epoch 1/16 - Train Loss: 0.6641, Val Loss: 0.8521 | Acc: 0.3448, F1: 0.1768, ROC-AUC: 0.5474
Epoch 2/16 - Train Loss: 0.6057, Val Loss: 0.9212 | Acc: 0.3448, F1: 0.1768, ROC-AUC: 0.5579
Epoch 3/16 - Train Loss: 0.6644, Val Loss: 0.8052 | Acc: 0.3448, F1: 0.1768, ROC-AUC: 0.5678
Epoch 4: Partially unfroze CLIP (last block + LayerNorms).
Epoch 4/16 - Train Loss: 0.6402, Val Loss: 0.8501 | Acc: 0.3448, F1: 0.1768, ROC-AUC: 0.6000
Epoch 5/16 - Train Loss: 0.5834, Val Loss: 0.9185 | Acc: 0.3448, F1: 0.1768, ROC-AUC: 0.5934
Epoch 6/16 - Train Loss: 0.6444, Val Loss: 0.7613 | Acc: 0.3793, F1: 0.2915, ROC-AUC: 0.5914
Epoch 7/16 - Train Loss: 0.6068, Val Loss: 0.8255 | Acc: 0.3966, F1: 0.2798, ROC-AUC: 0.5921
Epoch 8/16 - Train Loss: 0.6134, Val Loss: 0.8311 | Acc: 0.3966, F1: 0.2798, ROC-AUC: 0.5961
Epoch 9/16 - Train Loss: 0.6001, Val Loss: 0.8433 | Acc: 0.3793, F1: 0.2470, ROC-AUC: 0.6046
Epoch 10/16 - Train Loss: 0.5965, Val Loss: 0.8261 | Acc: 0.3966, F1: 0.2798, ROC-AUC: 0.5842
Epoch 11/16 - Train Loss: 0.6217, Val Loss: 0.8253 | Acc: 0.3966, F1: 0.2798, ROC-AUC: 0.5822
Epoch 12/16 - Train Loss: 0.6179, Val Loss: 0.8064 | Acc: 0.3621, F1: 0.2615, ROC-AUC: 0.5803
Epoch 13/16 - Train Loss: 0.6018, Val Loss: 0.8104 | Acc: 0.3793, F1: 0.2707, ROC-AUC: 0.5789
Epoch 14/16 - Train Loss: 0.6108, Val Loss: 0.8081 | Acc: 0.3793, F1: 0.2707, ROC-AUC: 0.5796
Early stopping at epoch 14
Fold 4 - Acc: 0.3793, F1: 0.2707, ROC-AUC: 0.5796

--- Fold 5/5 ---
Initializing model and data...
Fine-tuning CLIP: No (frozen for 3 epochs)
Batch size: 16
Trainable params: 132,866
Epoch 1/16 - Train Loss: 0.6750, Val Loss: 0.9464 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.2962
Epoch 2/16 - Train Loss: 0.6412, Val Loss: 0.8845 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.3198
Epoch 3/16 - Train Loss: 0.6327, Val Loss: 0.9618 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.3347
Epoch 4: Partially unfroze CLIP (last block + LayerNorms).
Epoch 4/16 - Train Loss: 0.6552, Val Loss: 0.7550 | Acc: 0.4138, F1: 0.4138, ROC-AUC: 0.3860
Epoch 5/16 - Train Loss: 0.6484, Val Loss: 0.9108 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.3981
Epoch 6/16 - Train Loss: 0.5797, Val Loss: 0.8978 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.4258
Epoch 7/16 - Train Loss: 0.6401, Val Loss: 0.7975 | Acc: 0.3793, F1: 0.2643, ROC-AUC: 0.4764
Epoch 8/16 - Train Loss: 0.6336, Val Loss: 0.7824 | Acc: 0.3966, F1: 0.3155, ROC-AUC: 0.4413
Epoch 9/16 - Train Loss: 0.6309, Val Loss: 0.8653 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.3961
Epoch 10/16 - Train Loss: 0.5987, Val Loss: 0.8541 | Acc: 0.3276, F1: 0.1617, ROC-AUC: 0.4217
Epoch 11/16 - Train Loss: 0.5931, Val Loss: 0.8342 | Acc: 0.3793, F1: 0.2643, ROC-AUC: 0.4291
Epoch 12/16 - Train Loss: 0.5973, Val Loss: 0.8628 | Acc: 0.3448, F1: 0.1974, ROC-AUC: 0.4345
Early stopping at epoch 12
Fold 5 - Acc: 0.3448, F1: 0.1974, ROC-AUC: 0.4345

==================================================
Mode: vision_only
Avg Accuracy: 0.3923, Avg F1: 0.2888, Avg ROC-AUC: 0.5246
==================================================

Training complete.



===== MODE: multimodal_basic =====
============================================================
[Hardware Environment Check]
============================================================
CUDA device count: 1
  - GPU 0: NVIDIA GeForce RTX 4050 Laptop GPU
Using device: NVIDIA GeForce RTX 4050 Laptop GPU
============================================================

Starting training: multimodal_basic on cuda


--- Fold 1/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 264,962
Epoch 1/20 - Train Loss: 0.6201, Val Loss: 0.9515 | Acc: 0.3559, F1: 0.2069, ROC-AUC: 0.7859
Epoch 2/20 - Train Loss: 0.5646, Val Loss: 0.7455 | Acc: 0.5424, F1: 0.5134, ROC-AUC: 0.7635
Epoch 3/20 - Train Loss: 0.4626, Val Loss: 0.6620 | Acc: 0.6780, F1: 0.6780, ROC-AUC: 0.8314
Epoch 4/20 - Train Loss: 0.4128, Val Loss: 0.8774 | Acc: 0.5593, F1: 0.5360, ROC-AUC: 0.8064
Epoch 5/20 - Train Loss: 0.4233, Val Loss: 0.5487 | Acc: 0.7288, F1: 0.7251, ROC-AUC: 0.8397
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4046, Val Loss: 0.5498 | Acc: 0.7797, F1: 0.7844, ROC-AUC: 0.8423
Epoch 7/20 - Train Loss: 0.4241, Val Loss: 0.5549 | Acc: 0.7627, F1: 0.7683, ROC-AUC: 0.8474
Epoch 8/20 - Train Loss: 0.3329, Val Loss: 0.5774 | Acc: 0.7288, F1: 0.7146, ROC-AUC: 0.8667
Epoch 9/20 - Train Loss: 0.3073, Val Loss: 0.6119 | Acc: 0.7966, F1: 0.8014, ROC-AUC: 0.8321
Epoch 10/20 - Train Loss: 0.2690, Val Loss: 0.5972 | Acc: 0.7458, F1: 0.7472, ROC-AUC: 0.8474
Epoch 11/20 - Train Loss: 0.2339, Val Loss: 0.6220 | Acc: 0.7797, F1: 0.7829, ROC-AUC: 0.8423
Epoch 12/20 - Train Loss: 0.1891, Val Loss: 0.6160 | Acc: 0.8136, F1: 0.8183, ROC-AUC: 0.8449
Epoch 13/20 - Train Loss: 0.1932, Val Loss: 0.7331 | Acc: 0.7458, F1: 0.7472, ROC-AUC: 0.8173
Early stopping at epoch 13
Fold 1 - Acc: 0.7458, F1: 0.7472, ROC-AUC: 0.8173

--- Fold 2/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 264,962
Epoch 1/20 - Train Loss: 0.6033, Val Loss: 0.7861 | Acc: 0.4237, F1: 0.3335, ROC-AUC: 0.7513
Epoch 2/20 - Train Loss: 0.4807, Val Loss: 0.6233 | Acc: 0.6102, F1: 0.6140, ROC-AUC: 0.7910
Epoch 3/20 - Train Loss: 0.4513, Val Loss: 0.5674 | Acc: 0.6949, F1: 0.7024, ROC-AUC: 0.8141
Epoch 4/20 - Train Loss: 0.3676, Val Loss: 0.5317 | Acc: 0.7119, F1: 0.7191, ROC-AUC: 0.8231
Epoch 5/20 - Train Loss: 0.3925, Val Loss: 0.7207 | Acc: 0.6271, F1: 0.6250, ROC-AUC: 0.8231
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4283, Val Loss: 0.7174 | Acc: 0.6271, F1: 0.6290, ROC-AUC: 0.8000
Epoch 7/20 - Train Loss: 0.3541, Val Loss: 0.6013 | Acc: 0.7288, F1: 0.7355, ROC-AUC: 0.8154
Epoch 8/20 - Train Loss: 0.3112, Val Loss: 0.5824 | Acc: 0.7288, F1: 0.7288, ROC-AUC: 0.8147
Epoch 9/20 - Train Loss: 0.2815, Val Loss: 0.6845 | Acc: 0.6610, F1: 0.6679, ROC-AUC: 0.8218
Epoch 10/20 - Train Loss: 0.2550, Val Loss: 0.6291 | Acc: 0.7119, F1: 0.7193, ROC-AUC: 0.8295
Epoch 11/20 - Train Loss: 0.2410, Val Loss: 0.5735 | Acc: 0.7458, F1: 0.7512, ROC-AUC: 0.8282
Epoch 12/20 - Train Loss: 0.2169, Val Loss: 0.6077 | Acc: 0.7288, F1: 0.7359, ROC-AUC: 0.8244
Early stopping at epoch 12
Fold 2 - Acc: 0.7288, F1: 0.7359, ROC-AUC: 0.8244

--- Fold 3/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 264,962
Epoch 1/20 - Train Loss: 0.6761, Val Loss: 0.8720 | Acc: 0.3729, F1: 0.2406, ROC-AUC: 0.7660
Epoch 2/20 - Train Loss: 0.5295, Val Loss: 0.8341 | Acc: 0.4915, F1: 0.4416, ROC-AUC: 0.7859
Epoch 3/20 - Train Loss: 0.4710, Val Loss: 0.7611 | Acc: 0.5593, F1: 0.5509, ROC-AUC: 0.7962
Epoch 4/20 - Train Loss: 0.4483, Val Loss: 0.5572 | Acc: 0.7288, F1: 0.7360, ROC-AUC: 0.8115
Epoch 5/20 - Train Loss: 0.4199, Val Loss: 0.5837 | Acc: 0.7627, F1: 0.7685, ROC-AUC: 0.8231
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4636, Val Loss: 0.7067 | Acc: 0.6780, F1: 0.6835, ROC-AUC: 0.8064
Epoch 7/20 - Train Loss: 0.4402, Val Loss: 0.7263 | Acc: 0.6271, F1: 0.6200, ROC-AUC: 0.8654
Epoch 8/20 - Train Loss: 0.3914, Val Loss: 0.6068 | Acc: 0.7288, F1: 0.7343, ROC-AUC: 0.8500
Epoch 9/20 - Train Loss: 0.2582, Val Loss: 0.6071 | Acc: 0.7458, F1: 0.7525, ROC-AUC: 0.8397
Epoch 10/20 - Train Loss: 0.2116, Val Loss: 0.6375 | Acc: 0.7458, F1: 0.7523, ROC-AUC: 0.8359
Epoch 11/20 - Train Loss: 0.2614, Val Loss: 0.5990 | Acc: 0.7458, F1: 0.7512, ROC-AUC: 0.8256
Epoch 12/20 - Train Loss: 0.2424, Val Loss: 0.6011 | Acc: 0.7458, F1: 0.7441, ROC-AUC: 0.8250
Early stopping at epoch 12
Fold 3 - Acc: 0.7458, F1: 0.7441, ROC-AUC: 0.8250

--- Fold 4/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 264,962
Epoch 1/20 - Train Loss: 0.6962, Val Loss: 0.7021 | Acc: 0.5862, F1: 0.5683, ROC-AUC: 0.7487
Epoch 2/20 - Train Loss: 0.5873, Val Loss: 0.5270 | Acc: 0.7759, F1: 0.7668, ROC-AUC: 0.8987
Epoch 3/20 - Train Loss: 0.5042, Val Loss: 0.5381 | Acc: 0.7759, F1: 0.7802, ROC-AUC: 0.9342
Epoch 4/20 - Train Loss: 0.4485, Val Loss: 0.4280 | Acc: 0.8448, F1: 0.8485, ROC-AUC: 0.9487
Epoch 5/20 - Train Loss: 0.3806, Val Loss: 0.5773 | Acc: 0.7414, F1: 0.7448, ROC-AUC: 0.9224
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4331, Val Loss: 0.7742 | Acc: 0.5862, F1: 0.5683, ROC-AUC: 0.9355
Epoch 7/20 - Train Loss: 0.4164, Val Loss: 0.5735 | Acc: 0.7069, F1: 0.7084, ROC-AUC: 0.9276
Epoch 8/20 - Train Loss: 0.3495, Val Loss: 0.5134 | Acc: 0.7931, F1: 0.7976, ROC-AUC: 0.9408
Epoch 9/20 - Train Loss: 0.2487, Val Loss: 0.4320 | Acc: 0.8448, F1: 0.8480, ROC-AUC: 0.9329
Epoch 10/20 - Train Loss: 0.2836, Val Loss: 0.5417 | Acc: 0.7759, F1: 0.7802, ROC-AUC: 0.9316
Epoch 11/20 - Train Loss: 0.2700, Val Loss: 0.3943 | Acc: 0.8793, F1: 0.8785, ROC-AUC: 0.9421
Epoch 12/20 - Train Loss: 0.2580, Val Loss: 0.3924 | Acc: 0.8448, F1: 0.8415, ROC-AUC: 0.9355
Epoch 13/20 - Train Loss: 0.2382, Val Loss: 0.3955 | Acc: 0.8793, F1: 0.8785, ROC-AUC: 0.9316
Epoch 14/20 - Train Loss: 0.2104, Val Loss: 0.4217 | Acc: 0.8276, F1: 0.8294, ROC-AUC: 0.9145
Epoch 15/20 - Train Loss: 0.2082, Val Loss: 0.4302 | Acc: 0.8448, F1: 0.8438, ROC-AUC: 0.9092
Epoch 16/20 - Train Loss: 0.1864, Val Loss: 0.4363 | Acc: 0.7759, F1: 0.7804, ROC-AUC: 0.9132
Epoch 17/20 - Train Loss: 0.1931, Val Loss: 0.4439 | Acc: 0.8276, F1: 0.8276, ROC-AUC: 0.9105
Epoch 18/20 - Train Loss: 0.1685, Val Loss: 0.4494 | Acc: 0.8103, F1: 0.8131, ROC-AUC: 0.9118
Epoch 19/20 - Train Loss: 0.1794, Val Loss: 0.4491 | Acc: 0.8103, F1: 0.8131, ROC-AUC: 0.9105
Epoch 20/20 - Train Loss: 0.1658, Val Loss: 0.4498 | Acc: 0.8103, F1: 0.8131, ROC-AUC: 0.9105
Early stopping at epoch 20
Fold 4 - Acc: 0.8103, F1: 0.8131, ROC-AUC: 0.9105

--- Fold 5/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 264,962
Epoch 1/20 - Train Loss: 0.6331, Val Loss: 0.8589 | Acc: 0.3621, F1: 0.2316, ROC-AUC: 0.6404
Epoch 2/20 - Train Loss: 0.5691, Val Loss: 0.6352 | Acc: 0.6379, F1: 0.6402, ROC-AUC: 0.6842
Epoch 3/20 - Train Loss: 0.4873, Val Loss: 0.8070 | Acc: 0.5862, F1: 0.5743, ROC-AUC: 0.7868
Epoch 4/20 - Train Loss: 0.4063, Val Loss: 0.5682 | Acc: 0.7586, F1: 0.7660, ROC-AUC: 0.8178
Epoch 5/20 - Train Loss: 0.3504, Val Loss: 0.5990 | Acc: 0.7586, F1: 0.7660, ROC-AUC: 0.8097
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4135, Val Loss: 0.6687 | Acc: 0.7414, F1: 0.7493, ROC-AUC: 0.7665
Epoch 7/20 - Train Loss: 0.3274, Val Loss: 0.7078 | Acc: 0.6724, F1: 0.6814, ROC-AUC: 0.7773
Epoch 8/20 - Train Loss: 0.3085, Val Loss: 0.5831 | Acc: 0.7414, F1: 0.7457, ROC-AUC: 0.8219
Epoch 9/20 - Train Loss: 0.3230, Val Loss: 0.8693 | Acc: 0.6379, F1: 0.6461, ROC-AUC: 0.7746
Epoch 10/20 - Train Loss: 0.2431, Val Loss: 0.8030 | Acc: 0.7069, F1: 0.7157, ROC-AUC: 0.7922
Epoch 11/20 - Train Loss: 0.2361, Val Loss: 0.8569 | Acc: 0.6724, F1: 0.6798, ROC-AUC: 0.8030
Epoch 12/20 - Train Loss: 0.2230, Val Loss: 0.8203 | Acc: 0.7241, F1: 0.7326, ROC-AUC: 0.7800
Early stopping at epoch 12
Fold 5 - Acc: 0.7241, F1: 0.7326, ROC-AUC: 0.7800

==================================================
Mode: multimodal_basic
Avg Accuracy: 0.7510, Avg F1: 0.7546, Avg ROC-AUC: 0.8314
==================================================

Training complete.



===== MODE: multimodal_full =====
============================================================
[Hardware Environment Check]
============================================================
CUDA device count: 1
  - GPU 0: NVIDIA GeForce RTX 4050 Laptop GPU
Using device: NVIDIA GeForce RTX 4050 Laptop GPU
============================================================

Starting training: multimodal_full on cuda


--- Fold 1/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 626,114
Epoch 1/20 - Train Loss: 0.6145, Val Loss: 0.5936 | Acc: 0.6610, F1: 0.6693, ROC-AUC: 0.8205
Epoch 2/20 - Train Loss: 0.5636, Val Loss: 0.6158 | Acc: 0.6610, F1: 0.6657, ROC-AUC: 0.8321
Epoch 3/20 - Train Loss: 0.5196, Val Loss: 0.5558 | Acc: 0.6949, F1: 0.7024, ROC-AUC: 0.8500
Epoch 4/20 - Train Loss: 0.4119, Val Loss: 0.6265 | Acc: 0.6949, F1: 0.6991, ROC-AUC: 0.8756
Epoch 5/20 - Train Loss: 0.4023, Val Loss: 0.7837 | Acc: 0.6102, F1: 0.5998, ROC-AUC: 0.9000
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.3698, Val Loss: 0.5282 | Acc: 0.7627, F1: 0.7671, ROC-AUC: 0.8603
Epoch 7/20 - Train Loss: 0.3677, Val Loss: 0.5381 | Acc: 0.7966, F1: 0.8019, ROC-AUC: 0.8833
Epoch 8/20 - Train Loss: 0.2921, Val Loss: 0.5644 | Acc: 0.7797, F1: 0.7844, ROC-AUC: 0.8436
Epoch 9/20 - Train Loss: 0.2943, Val Loss: 0.6010 | Acc: 0.7797, F1: 0.7829, ROC-AUC: 0.8449
Epoch 10/20 - Train Loss: 0.2289, Val Loss: 0.9301 | Acc: 0.6780, F1: 0.6811, ROC-AUC: 0.8615
Epoch 11/20 - Train Loss: 0.4102, Val Loss: 0.6260 | Acc: 0.7966, F1: 0.8020, ROC-AUC: 0.8641
Epoch 12/20 - Train Loss: 0.2835, Val Loss: 0.7184 | Acc: 0.7797, F1: 0.7855, ROC-AUC: 0.8423
Epoch 13/20 - Train Loss: 0.2208, Val Loss: 0.6928 | Acc: 0.7797, F1: 0.7829, ROC-AUC: 0.8410
Epoch 14/20 - Train Loss: 0.2083, Val Loss: 0.6400 | Acc: 0.7966, F1: 0.7988, ROC-AUC: 0.8359
Early stopping at epoch 14
Fold 1 - Acc: 0.7966, F1: 0.7988, ROC-AUC: 0.8359

--- Fold 2/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 626,114
Epoch 1/20 - Train Loss: 0.6011, Val Loss: 0.9065 | Acc: 0.4576, F1: 0.3895, ROC-AUC: 0.7500
Epoch 2/20 - Train Loss: 0.4785, Val Loss: 0.6082 | Acc: 0.7288, F1: 0.7355, ROC-AUC: 0.8115
Epoch 3/20 - Train Loss: 0.3905, Val Loss: 0.7746 | Acc: 0.7119, F1: 0.7184, ROC-AUC: 0.7885
Epoch 4/20 - Train Loss: 0.4683, Val Loss: 0.5321 | Acc: 0.7797, F1: 0.7782, ROC-AUC: 0.8115
Epoch 5/20 - Train Loss: 0.3976, Val Loss: 0.6110 | Acc: 0.7119, F1: 0.7191, ROC-AUC: 0.8128
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.3652, Val Loss: 0.6675 | Acc: 0.6949, F1: 0.7011, ROC-AUC: 0.8167
Epoch 7/20 - Train Loss: 0.4145, Val Loss: 0.6708 | Acc: 0.7458, F1: 0.7522, ROC-AUC: 0.7962
Epoch 8/20 - Train Loss: 0.2938, Val Loss: 0.6461 | Acc: 0.7119, F1: 0.7193, ROC-AUC: 0.8372
Epoch 9/20 - Train Loss: 0.2863, Val Loss: 0.5716 | Acc: 0.7797, F1: 0.7852, ROC-AUC: 0.8487
Epoch 10/20 - Train Loss: 0.1721, Val Loss: 0.6721 | Acc: 0.7966, F1: 0.7988, ROC-AUC: 0.8308
Epoch 11/20 - Train Loss: 0.2307, Val Loss: 0.6014 | Acc: 0.8305, F1: 0.8323, ROC-AUC: 0.8397
Epoch 12/20 - Train Loss: 0.2535, Val Loss: 0.5854 | Acc: 0.7797, F1: 0.7844, ROC-AUC: 0.8474
Early stopping at epoch 12
Fold 2 - Acc: 0.7797, F1: 0.7844, ROC-AUC: 0.8474

--- Fold 3/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 626,114
Epoch 1/20 - Train Loss: 0.5111, Val Loss: 0.7740 | Acc: 0.5593, F1: 0.5440, ROC-AUC: 0.8641
Epoch 2/20 - Train Loss: 0.5473, Val Loss: 0.7322 | Acc: 0.5763, F1: 0.5578, ROC-AUC: 0.8897
Epoch 3/20 - Train Loss: 0.4806, Val Loss: 0.4488 | Acc: 0.8305, F1: 0.8345, ROC-AUC: 0.9141
Epoch 4/20 - Train Loss: 0.4387, Val Loss: 0.4689 | Acc: 0.8305, F1: 0.8323, ROC-AUC: 0.9071
Epoch 5/20 - Train Loss: 0.4274, Val Loss: 0.4306 | Acc: 0.8136, F1: 0.8123, ROC-AUC: 0.9192
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.3581, Val Loss: 0.4851 | Acc: 0.8136, F1: 0.8059, ROC-AUC: 0.8885
Epoch 7/20 - Train Loss: 0.3208, Val Loss: 0.6304 | Acc: 0.7627, F1: 0.7689, ROC-AUC: 0.8590
Epoch 8/20 - Train Loss: 0.2763, Val Loss: 0.7013 | Acc: 0.7119, F1: 0.7195, ROC-AUC: 0.8308
Epoch 9/20 - Train Loss: 0.2654, Val Loss: 0.5780 | Acc: 0.7288, F1: 0.7359, ROC-AUC: 0.8647
Epoch 10/20 - Train Loss: 0.1927, Val Loss: 0.6296 | Acc: 0.7458, F1: 0.7512, ROC-AUC: 0.8513
Epoch 11/20 - Train Loss: 0.1850, Val Loss: 0.6536 | Acc: 0.7627, F1: 0.7671, ROC-AUC: 0.8744
Epoch 12/20 - Train Loss: 0.2426, Val Loss: 0.6666 | Acc: 0.7288, F1: 0.7251, ROC-AUC: 0.8308
Epoch 13/20 - Train Loss: 0.2035, Val Loss: 0.7082 | Acc: 0.7627, F1: 0.7671, ROC-AUC: 0.7897
Early stopping at epoch 13
Fold 3 - Acc: 0.7627, F1: 0.7671, ROC-AUC: 0.7897

--- Fold 4/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 626,114
Epoch 1/20 - Train Loss: 0.5641, Val Loss: 0.8474 | Acc: 0.5345, F1: 0.5099, ROC-AUC: 0.8434
Epoch 2/20 - Train Loss: 0.5058, Val Loss: 0.8499 | Acc: 0.6207, F1: 0.6162, ROC-AUC: 0.8408
Epoch 3/20 - Train Loss: 0.5365, Val Loss: 0.7465 | Acc: 0.6034, F1: 0.5898, ROC-AUC: 0.8645
Epoch 4/20 - Train Loss: 0.4154, Val Loss: 0.5660 | Acc: 0.7931, F1: 0.7981, ROC-AUC: 0.8697
Epoch 5/20 - Train Loss: 0.4317, Val Loss: 0.5161 | Acc: 0.7759, F1: 0.7814, ROC-AUC: 0.8836
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.4006, Val Loss: 0.4505 | Acc: 0.8448, F1: 0.8471, ROC-AUC: 0.8908
Epoch 7/20 - Train Loss: 0.4287, Val Loss: 0.4613 | Acc: 0.8448, F1: 0.8485, ROC-AUC: 0.8855
Epoch 8/20 - Train Loss: 0.3191, Val Loss: 0.4947 | Acc: 0.8103, F1: 0.8114, ROC-AUC: 0.8803
Epoch 9/20 - Train Loss: 0.3469, Val Loss: 0.4650 | Acc: 0.7931, F1: 0.7982, ROC-AUC: 0.9026
Epoch 10/20 - Train Loss: 0.2945, Val Loss: 0.6546 | Acc: 0.7759, F1: 0.7802, ROC-AUC: 0.8868
Epoch 11/20 - Train Loss: 0.2527, Val Loss: 0.5776 | Acc: 0.7586, F1: 0.7627, ROC-AUC: 0.9039
Epoch 12/20 - Train Loss: 0.2581, Val Loss: 0.7140 | Acc: 0.7759, F1: 0.7802, ROC-AUC: 0.8816
Epoch 13/20 - Train Loss: 0.2137, Val Loss: 0.7321 | Acc: 0.7586, F1: 0.7645, ROC-AUC: 0.8934
Epoch 14/20 - Train Loss: 0.1825, Val Loss: 0.5973 | Acc: 0.7931, F1: 0.7977, ROC-AUC: 0.8842
Early stopping at epoch 14
Fold 4 - Acc: 0.7931, F1: 0.7977, ROC-AUC: 0.8842

--- Fold 5/5 ---
Initializing model and data...
C:\Users\petar\PycharmProjects\DementiaCLIP\.venv\Lib\site-packages\open_clip\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=True) and pretrained tag 'laion2b_s34b_b79k' (quick_gelu=False).
  warnings.warn(
Fine-tuning CLIP: No (frozen for 5 epochs)
Batch size: 16
Trainable params: 626,114
Epoch 1/20 - Train Loss: 0.5969, Val Loss: 0.5892 | Acc: 0.7241, F1: 0.7002, ROC-AUC: 0.7233
Epoch 2/20 - Train Loss: 0.4785, Val Loss: 0.6278 | Acc: 0.7241, F1: 0.6906, ROC-AUC: 0.7443
Epoch 3/20 - Train Loss: 0.4556, Val Loss: 0.5555 | Acc: 0.7586, F1: 0.7446, ROC-AUC: 0.7962
Epoch 4/20 - Train Loss: 0.4763, Val Loss: 0.5530 | Acc: 0.7759, F1: 0.7704, ROC-AUC: 0.8057
Epoch 5/20 - Train Loss: 0.3962, Val Loss: 0.5516 | Acc: 0.7414, F1: 0.7395, ROC-AUC: 0.8023
Epoch 6: Partially unfroze CLIP (last block + LayerNorms).
Epoch 6/20 - Train Loss: 0.3487, Val Loss: 0.5468 | Acc: 0.7759, F1: 0.7812, ROC-AUC: 0.8259
Epoch 7/20 - Train Loss: 0.3459, Val Loss: 0.5674 | Acc: 0.7586, F1: 0.7636, ROC-AUC: 0.8259
Epoch 8/20 - Train Loss: 0.3118, Val Loss: 0.6461 | Acc: 0.7414, F1: 0.7493, ROC-AUC: 0.8435
Epoch 9/20 - Train Loss: 0.2626, Val Loss: 0.7038 | Acc: 0.7241, F1: 0.7315, ROC-AUC: 0.8070
Epoch 10/20 - Train Loss: 0.2816, Val Loss: 0.6484 | Acc: 0.7414, F1: 0.7476, ROC-AUC: 0.8084
Epoch 11/20 - Train Loss: 0.1872, Val Loss: 0.6532 | Acc: 0.7586, F1: 0.7650, ROC-AUC: 0.8502
Epoch 12/20 - Train Loss: 0.1887, Val Loss: 0.6769 | Acc: 0.7586, F1: 0.7650, ROC-AUC: 0.8286
Epoch 13/20 - Train Loss: 0.1630, Val Loss: 0.6065 | Acc: 0.7931, F1: 0.7956, ROC-AUC: 0.8414
Epoch 14/20 - Train Loss: 0.1645, Val Loss: 0.6862 | Acc: 0.7759, F1: 0.7743, ROC-AUC: 0.8124
Early stopping at epoch 14
Fold 5 - Acc: 0.7759, F1: 0.7743, ROC-AUC: 0.8124

==================================================
Mode: multimodal_full
Avg Accuracy: 0.7816, Avg F1: 0.7844, Avg ROC-AUC: 0.8339
==================================================

Training complete.

